{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bd5a53",
   "metadata": {},
   "source": [
    "## 1- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "944bcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2a0c35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "73e090f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.read_csv(\"../Data/processed/dfd_cle_RR_viz_pow_dr_enc_feng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7d73af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \n",
    "    # iterate through all the columns of a dataframe and modify the data type\n",
    "    #   to reduce memory usage.        \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "47558afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 872.57 MB\n",
      "Memory usage after optimization is: 213.95 MB\n",
      "Decreased by 75.5%\n"
     ]
    }
   ],
   "source": [
    "history_df = reduce_mem_usage(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b74f59",
   "metadata": {},
   "source": [
    "## 2- Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "edba20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4398809, 26)\n",
      "(4398809, 26)\n"
     ]
    }
   ],
   "source": [
    "print(history_df.shape)\n",
    "history_df_frac = history_df \n",
    "print(history_df_frac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3e685ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = history_df_frac.drop(['winPlacePerc'], axis=1)\n",
    "y = history_df_frac['winPlacePerc']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "03231aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3519047, 25), (3519047,), (879762, 25), (879762,))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3797e",
   "metadata": {},
   "source": [
    "## 3- Model Building "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb16ba",
   "metadata": {},
   "source": [
    "**NOTE** even though we reduced the skewness of the data by removing a significant number of outliers, the data is still fairly skewed and there still number of data points that can be considered outliers, the main reason for not removing all the potential outliers was to not cut too much from the data as to affect the predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ef06c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "# from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "# from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cfcd2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "rmse_cv = {}\n",
    "acc_cv = {}\n",
    "\n",
    "def rmsle_cv(model, model_name):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    acc = cross_val_score(model, X_train.values, y_train, cv = kf)\n",
    "    rmse_cv[model_name] = rmse.mean()\n",
    "    acc_cv[model_name] = acc.mean()\n",
    "    \n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df177648",
   "metadata": {},
   "source": [
    "### 3.1 linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f6602",
   "metadata": {},
   "source": [
    "this model is sensitive to outliers, so we use the sklearn's Robustscaler() method on pipeline to account for outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aadcf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = make_pipeline(RobustScaler(), LinearRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f2ebd5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear score: 0.1441 (0.0012)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(linear, 'Linear Regression')\n",
    "print(\"\\nLinear score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edea37c",
   "metadata": {},
   "source": [
    "### 3.2 Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba0a97",
   "metadata": {},
   "source": [
    "this model is sensitive to outliers, so we use the sklearn's Robustscaler() method on pipeline to account for outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "44ed7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dd539e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 0.1381 (0.0001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(lasso, 'Lasso Regression')\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9456c7b",
   "metadata": {},
   "source": [
    "### 3.3 Elastic Net "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2829b67",
   "metadata": {},
   "source": [
    "this model is sensitive to outliers, so we use the sklearn's Robustscaler() method on pipeline to account for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8119f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "06435f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elastic Net score: 0.1380 (0.0001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(ENet, 'Elastic Net')\n",
    "print(\"\\nElastic Net score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39178d",
   "metadata": {},
   "source": [
    "### 3.4 Kernel Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78c958",
   "metadata": {},
   "source": [
    "Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a6a4893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "64b34050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(KRR)\n",
    "# print(\"\\nKernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d1500",
   "metadata": {},
   "source": [
    "**NOTE** this model takes too much memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74a193",
   "metadata": {},
   "source": [
    "### 3.5 Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a834f0a",
   "metadata": {},
   "source": [
    "With huber loss that makes it robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "68600c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "24e845f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(GBoost)\n",
    "# print(\"\\nGradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f2eec",
   "metadata": {},
   "source": [
    "**NOTE** model takes too long in training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63220367",
   "metadata": {},
   "source": [
    "### 3.6 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "543b2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3f2f5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(model_xgb)\n",
    "# print(\"\\nExtreme Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f02121",
   "metadata": {},
   "source": [
    "**NOTE** model takes too long in training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc48569",
   "metadata": {},
   "source": [
    "### 3.7 LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ee8c8",
   "metadata": {},
   "source": [
    "gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cf09b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "01fdd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         \"objective\" : \"regression\", \n",
    "#         \"metric\" : \"mae\", \n",
    "#         \"num_leaves\" : 149, \n",
    "#         \"learning_rate\" : 0.03, \n",
    "#         \"bagging_fraction\" : 0.9,\n",
    "#         \"bagging_seed\" : 0, \n",
    "#         \"num_threads\" : 4,\n",
    "#         \"colsample_bytree\" : 0.5,\n",
    "#         'min_data_in_leaf':1900, \n",
    "#         'min_split_gain':0.00011,\n",
    "#         'lambda_l2':9\n",
    "# }\n",
    "\n",
    "# model = lgb.train(  params, \n",
    "#                     train_set = train_set,\n",
    "#                     num_boost_round=9400,\n",
    "#                     early_stopping_rounds=200,\n",
    "#                     verbose_eval=100, \n",
    "#                     valid_sets=[train_set,valid_set]\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fcab210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "\n",
      "Light Gradient Boosting score: 0.0796 (0.0001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_lgb, 'Light Gradient Boosting')\n",
    "print(\"\\nLight Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297d1d6",
   "metadata": {},
   "source": [
    "### 3.8 SVM regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7ee7b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMR = SVR(C=1.0, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b84ba0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(SVMR)\n",
    "# print(\"\\nLight Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e9c5b",
   "metadata": {},
   "source": [
    "**NOTE** model takes too long in training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ae252",
   "metadata": {},
   "source": [
    "### 3.9 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7b47b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "35b63b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree score: 0.1016 (0.0001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(DT, 'Decision Tree')\n",
    "print(\"\\nDecision Tree score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538742e",
   "metadata": {},
   "source": [
    "### 3.10 random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8b1339a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "randf = RandomForestRegressor(max_depth=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(randf, 'Random Forest')\n",
    "print(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a40565",
   "metadata": {},
   "source": [
    "**NOTE** model takes too long in training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa91fb6",
   "metadata": {},
   "source": [
    "### 3.11 Bayesian Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = DotProduct() + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9faff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(gpr)\n",
    "# print(\"\\nBayesian Regression score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15902802",
   "metadata": {},
   "source": [
    "**NOTE** model takes too much memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7994d1",
   "metadata": {},
   "source": [
    "### 3.12 KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ca48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = rmsle_cv(neigh)\n",
    "# print(\"\\nK-Nearest Neighbors score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30630b5e",
   "metadata": {},
   "source": [
    "**NOTE** takes too long to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e24cc2",
   "metadata": {},
   "source": [
    "## 4- Model Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c2f54",
   "metadata": {},
   "source": [
    "**metrics to use**\n",
    "- root mean squared error\n",
    "- mean squared error\n",
    "- mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3241ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = {}\n",
    "\n",
    "def show_rmse(predicted, expected):\n",
    "    # calculate errors\n",
    "    errors = list()\n",
    "    for i in range(len(expected)):\n",
    "        # calculate error\n",
    "        err = (expected[i] - predicted[i])**2\n",
    "        # store error\n",
    "        errors.append(err)\n",
    "    #     # report error\n",
    "    #     print('>%.1f, %.1f = %.3f' % (expected[i], predicted[i], err))\n",
    "\n",
    "    # plot errors\n",
    "    plt.plot(errors)\n",
    "    plt.xticks(ticks=[i for i in range(len(errors))], labels=predicted)\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.show()\n",
    "\n",
    "def evaluating_predictions(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    expected = y_test.reset_index(drop=True)\n",
    "    predicted = model.predict(X_test)\n",
    "    \n",
    "    RMSE = mean_squared_error(expected, predicted, squared=False)\n",
    "    MSE = mean_squared_error(expected, predicted)\n",
    "    MAE = mean_absolute_error(expected, predicted)\n",
    "    \n",
    "    global eval_scores\n",
    "    eval_scores[model_name] = {}\n",
    "    eval_scores[model_name]['RMSE'] = RMSE\n",
    "    eval_scores[model_name]['MSE'] = MSE\n",
    "    eval_scores[model_name]['MAE'] = MAE\n",
    "    \n",
    "    print('model {} ---> RMSE: {:.5f} / MSE: {:.5f} / MAE: {:.5f}'\n",
    "          .format(model_name, RMSE, MSE, MAE))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a36d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluating_predictions(linear, 'Linear Regression')\n",
    "evaluating_predictions(lasso, 'Lasso Regression')\n",
    "evaluating_predictions(ENet, 'Elastic Net')\n",
    "# evaluating_predictions(KRR, 'Kernel Ridge Regression')  # takes too much memory\n",
    "# evaluating_predictions(GBoost, 'Gradient Boosting')  # takes too long\n",
    "# evaluating_predictions(model_xgb, 'Extreme Gradient Boosting')  # takes too long\n",
    "evaluating_predictions(model_lgb, 'Light Gradient Boosting')\n",
    "# evaluating_predictions(SVMR, 'Support Vector Machine')  # takes too long\n",
    "evaluating_predictions(DT, 'Decision Tree')\n",
    "evaluating_predictions(randf, 'Random Forest')\n",
    "# evaluating_predictions(gpr, 'Gaussian Process Regressor')  # takes too much memory\n",
    "# evaluating_predictions(neigh, 'K-Nearest Neigbors')  # takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3e5a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# making of dataframe for clearer view of overall results\n",
    "models = []\n",
    "RMSE = []\n",
    "MSE = []\n",
    "MAE = []\n",
    "RMSE_cv = []\n",
    "ACC_cv = []\n",
    "\n",
    "for models_name, eval_score in eval_scores.items():\n",
    "    models.append(models_name)\n",
    "    RMSE.append(eval_score['RMSE'])\n",
    "    MSE.append(eval_score['MSE'])\n",
    "    MAE.append(eval_score['MAE'])\n",
    "\n",
    "for models_name, eval_score_cv in rmse_cv.items():\n",
    "    RMSE_cv.append(rmse_cv[models_name])\n",
    "    \n",
    "for models_name, eval_score_cv in acc_cv.items():\n",
    "    ACC_cv.append(acc_cv[models_name])\n",
    "\n",
    "eval_results_df = df = pd.DataFrame(list(zip(RMSE, MSE, MAE, RMSE_cv)),\n",
    "               columns =['RMSE', 'MSE', 'MAE', 'RMSE_cv'], index=models)\n",
    "\n",
    "eval_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d888b9c",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0fec39",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "- https://towardsdatascience.com/what-are-rmse-and-mae-e405ce230383#:~:text=Technically%2C%20RMSE%20is%20the%20Root,actual%20values%20of%20a%20variable.\n",
    "- https://machinelearningmastery.com/regression-metrics-for-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataEngineeringKER",
   "language": "python",
   "name": "dataengineeringker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
